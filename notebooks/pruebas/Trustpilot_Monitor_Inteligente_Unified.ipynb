{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ Monitor Inteligente de Opini√≥n: Trustpilot Unified\n",
                "## üë• Proyecto End-to-End de Data Science - Herramienta de Inteligencia de Mercado\n",
                "\n",
                "Este notebook unifica las mejores funcionalidades de los prototipos anteriores para ofrecer una herramienta robusta de an√°lisis de sentimiento comercial.\n",
                "\n",
                "### ‚ú® Novedades y Autor√≠a:\n",
                "1.  **B√∫squeda Din√°mica (Rub√©n)**: Introduce el nombre de una empresa y el scraper encontrar√° el enlace autom√°ticamente.\n",
                "2.  **Modo Stealth Pro (Rub√©n)**: Integraci√≥n de rotaci√≥n de User-Agents y retardos aleatorios para evitar bloqueos.\n",
                "3.  **An√°lisis Combinado (Rub√©n)**: NLP avanzado con diccionarios locales de espa√±ol y modelos de polaridad.\n",
                "4.  **BI Dashboard (Juanes/Rub√©n)**: Visualizaciones interactivas y est√°ticas para reportes de negocio.\n",
                "\n",
                "**Metodolog√≠a:**\n",
                "1. üì• **FASE 1**: Adquisici√≥n de Datos (Web Scraping Inteligente)\n",
                "2. üßπ **FASE 2**: Preprocesamiento NLP (Limpieza y Lematizaci√≥n)\n",
                "3. üíé **FASE 3**: Extracci√≥n de Valor (Sentimiento y Frecuencia)\n",
                "4. üìä **FASE 4**: Visualizaci√≥n de Impacto (BI)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5947d49d",
            "metadata": {
                "id": "config"
            },
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# FASE 0: CONFIGURACI√ìN DEL ENTORNO\n",
                "# [Original + Mejora de Compatibilidad Windows: Rub√©n]\n",
                "# =============================================================================\n",
                "import sys\n",
                "import subprocess\n",
                "\n",
                "print(\"üîß Configurando entorno de ejecuci√≥n...\")\n",
                "\n",
                "def install_packages():\n",
                "    packages = [\n",
                "        \"requests\", \"beautifulsoup4\", \"lxml\", \"nltk\", \"textblob\", \n",
                "        \"googletrans==4.0.0-rc1\", \"wordcloud\", \"matplotlib\", \n",
                "        \"seaborn\", \"plotly\", \"fake-useragent\", \"pandas\", \n",
                "        \"numpy\", \"regex\", \"tqdm\", \"spacy\"\n",
                "    ]\n",
                "    \n",
                "    print(\"üì¶ Instalando librer√≠as necesarias (esto puede tardar unos minutos)...\")\n",
                "    for package in packages:\n",
                "        try:\n",
                "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è Error al instalar {package}: {e}\")\n",
                "    \n",
                "    # Descargar modelo de Spacy para espa√±ol\n",
                "    try:\n",
                "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_sm\", \"-q\"])\n",
                "    except:\n",
                "        print(\"‚ö†Ô∏è No se pudo descargar el modelo de spacy autom√°ticamente.\")\n",
                "\n",
                "install_packages()\n",
                "\n",
                "import nltk\n",
                "import spacy\n",
                "\n",
                "print(\"üì• Descargando recursos ling√º√≠sticos...\")\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "print(\"‚úÖ Entorno listo.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "imports"
            },
            "outputs": [],
            "source": [
                "# [Original + Imports Rub√©n para funcionalidad Stealth]\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import requests\n",
                "import re\n",
                "import time\n",
                "import random\n",
                "import json\n",
                "from datetime import datetime\n",
                "from typing import List, Dict, Optional\n",
                "from collections import Counter\n",
                "from bs4 import BeautifulSoup\n",
                "from fake_useragent import UserAgent\n",
                "from textblob import TextBlob\n",
                "from googletrans import Translator\n",
                "from wordcloud import WordCloud\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "plt.style.use('seaborn-v0_8')\n",
                "sns.set_palette(\"deep\")\n",
                "print(\"üìö Librer√≠as importadas correctamente.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì• FASE 1: ADQUISICI√ìN DE DATOS (SCRAPER INTELIGENTE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "scraper-class"
            },
            "outputs": [],
            "source": [
                "class TrustpilotScraper:\n",
                "    \"\"\"\n",
                "    Scraper avanzado mejorado para robustez y anonimato.\n",
                "    [Base: Correcci√≥n_TrustPilot | Mejoras: Rub√©n]\n",
                "    \"\"\"\n",
                "    def __init__(self, business_query: str = None, max_pages: int = 5):\n",
                "        self.query = business_query\n",
                "        self.max_pages = max_pages\n",
                "        self.ua = UserAgent() # [Rub√©n] Integraci√≥n de UA din√°mica\n",
                "        self.session = requests.Session()\n",
                "        self.base_url = \"https://es.trustpilot.com\"\n",
                "        self.target_url = None\n",
                "        self.reviews_data = []\n",
                "\n",
                "    def _get_headers(self):\n",
                "        \"\"\"[Rub√©n] Genera headers aleatorios para cada petici√≥n (Modo Stealth).\"\"\"\n",
                "        return {\n",
                "            'User-Agent': self.ua.random,\n",
                "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
                "            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',\n",
                "            'Referer': 'https://www.google.com/',\n",
                "            'DNT': '1',\n",
                "            'Connection': 'keep-alive'\n",
                "        }\n",
                "\n",
                "    def search_business(self) -> bool:\n",
                "        \"\"\"[Rub√©n] Busca la URL de la empresa autom√°ticamente.\"\"\"\n",
                "        if not self.query:\n",
                "            return False\n",
                "        \n",
                "        if \"trustpilot.com/review/\" in self.query:\n",
                "            self.target_url = self.query\n",
                "            return True\n",
                "\n",
                "        print(f\"üîç Buscando empresa: '{self.query}' en Trustpilot...\")\n",
                "        search_url = f\"{self.base_url}/search?query={self.query.replace(' ', '+')}\"\n",
                "        \n",
                "        try:\n",
                "            response = self.session.get(search_url, headers=self._get_headers(), timeout=15)\n",
                "            soup = BeautifulSoup(response.content, 'html.parser')\n",
                "            link_element = soup.select_one('a[data-business-unit-card-link=\"true\"]')\n",
                "            if link_element and link_element.has_attr('href'):\n",
                "                path = link_element['href']\n",
                "                self.target_url = self.base_url + path\n",
                "                print(f\"‚úÖ Enlace encontrado: {self.target_url}\")\n",
                "                return True\n",
                "            else:\n",
                "                print(\"‚ùå No se encontr√≥ la empresa. Intenta con la URL directa.\")\n",
                "                return False\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è Error durante la b√∫squeda: {e}\")\n",
                "            return False\n",
                "\n",
                "    def safe_extract(self, element, select_list: List[str], attr: str = None) -> str:\n",
                "        \"\"\"[Base: Correcci√≥n_TrustPilot] Prueba m√∫ltiples selectores para extraer un dato.\"\"\"\n",
                "        for selector in select_list:\n",
                "            found = element.select_one(selector)\n",
                "            if found:\n",
                "                if attr:\n",
                "                    return found.get(attr, \"\")\n",
                "                return found.get_text(strip=True)\n",
                "        return \"\"\n",
                "\n",
                "    def extract_review_info(self, card) -> Optional[Dict]:\n",
                "        \"\"\"Extrae los campos de una tarjeta de rese√±a individual. [Base: Correcci√≥n | Mejoras Fecha: Rub√©n]\"\"\"\n",
                "        txt_selectors = [\n",
                "            'p[data-service-review-text-typography=\"true\"]',\n",
                "            'p[data-relevant-review-text-typography=\"true\"]',\n",
                "            'p[data-review-content-typography=\"true\"]',\n",
                "            '.styles_reviewContent__0_vST p'\n",
                "        ]\n",
                "        \n",
                "        star_selectors = ['.styles_reviewHeader__iU9_n img', 'div[data-star-rating]']\n",
                "        date_selectors = ['time', 'span[data-service-review-date-time-ago]']\n",
                "\n",
                "        text = self.safe_extract(card, txt_selectors)\n",
                "        rating_raw = self.safe_extract(card, star_selectors, 'alt')\n",
                "        rating = re.search(r'\\d', rating_raw).group() if re.search(r'\\d', rating_raw) else \"0\"\n",
                "        date_raw = self.safe_extract(card, date_selectors, 'datetime')\n",
                "        \n",
                "        # [Rub√©n] Manejo avanzado de fechas ISO\n",
                "        try:\n",
                "            if date_raw:\n",
                "                date_obj = datetime.fromisoformat(date_raw.replace(\"Z\", \"+00:00\"))\n",
                "                date_str = date_obj.strftime('%Y-%m-%d')\n",
                "            else:\n",
                "                date_str = datetime.now().strftime('%Y-%m-%d')\n",
                "        except:\n",
                "            date_str = datetime.now().strftime('%Y-%m-%d')\n",
                "\n",
                "        if not text or len(text) < 10:\n",
                "            return None\n",
                "\n",
                "        return {\n",
                "            'texto': text,\n",
                "            'puntuacion': int(rating),\n",
                "            'fecha': date_str,\n",
                "            'longitud': len(text.split())\n",
                "        }\n",
                "\n",
                "    def run(self) -> pd.DataFrame:\n",
                "        \"\"\"[Integraci√≥n Rub√©n: Flujo completo con delays aleatorios]\"\"\"\n",
                "        if not self.target_url and not self.search_business():\n",
                "            return pd.DataFrame()\n",
                "\n",
                "        print(f\"üöÄ Iniciando extracci√≥n desde: {self.target_url}\")\n",
                "        \n",
                "        for page in range(1, self.max_pages + 1):\n",
                "            url = f\"{self.target_url}?page={page}\"\n",
                "            print(f\"üìÑ Procesando p√°gina {page}/{self.max_pages}...\")\n",
                "            \n",
                "            try:\n",
                "                # [Rub√©n] Delay aleatorio antidetect\n",
                "                time.sleep(random.uniform(2, 4))\n",
                "                \n",
                "                response = self.session.get(url, headers=self._get_headers(), timeout=20)\n",
                "                if response.status_code != 200:\n",
                "                    print(f\"‚ö†Ô∏è Error {response.status_code} al acceder a la p√°gina {page}\")\n",
                "                    break\n",
                "\n",
                "                soup = BeautifulSoup(response.content, 'html.parser')\n",
                "                cards = soup.select('article[data-service-review-card-paper=\"true\"]')\n",
                "                \n",
                "                if not cards:\n",
                "                    print(\"üèÅ No se encontraron m√°s rese√±as.\")\n",
                "                    break\n",
                "                \n",
                "                for card in cards:\n",
                "                    data = self.extract_review_info(card)\n",
                "                    if data:\n",
                "                        self.reviews_data.append(data)\n",
                "                \n",
                "                print(f\"‚úÖ {len(cards)} elementos analizados. Total acumulado: {len(self.reviews_data)}\")\n",
                "                \n",
                "            except Exception as e:\n",
                "                print(f\"‚ùå Error cr√≠tico en p√°gina {page}: {e}\")\n",
                "                break\n",
                "        \n",
                "        df = pd.DataFrame(self.reviews_data)\n",
                "        if not df.empty:\n",
                "            df = df.drop_duplicates(subset=['texto'])\n",
                "            print(f\"\\nüìä Extracci√≥n finalizada: {len(df)} rese√±as √∫nicas obtenidas.\")\n",
                "        return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run-scraper"
            },
            "outputs": [],
            "source": [
                "# [Rub√©n: Interfaz de entrada din√°mica]\n",
                "print(\"üîé CONFIGURACI√ìN DE B√öSQUEDA\")\n",
                "print(\"Escribe el nombre de la empresa (ej: 'Amazon Spain') o pega la URL de Trustpilot:\")\n",
                "target = input(\"> \").strip() \n",
                "\n",
                "if not target:\n",
                "    target = \"Amazon Spain\" \n",
                "    print(f\"Usando valor por defecto: {target}\")\n",
                "\n",
                "# [Juanes: L√≥gica de ejecuci√≥n del scraper]\n",
                "scraper = TrustpilotScraper(business_query=target, max_pages=3)\n",
                "df_raw = scraper.run()\n",
                "\n",
                "if df_raw.empty:\n",
                "    print(\"üîÑ Cargando dataset de respaldo para demostraci√≥n...\")\n",
                "    df_raw = pd.DataFrame({\n",
                "        'texto': [\n",
                "            \"Excelente servicio, el paquete lleg√≥ antes de lo esperado. Muy contento.\",\n",
                "            \"P√©sima atenci√≥n al cliente. Mi pedido nunca lleg√≥ y nadie responde.\",\n",
                "            \"Producto de buena calidad, pero el env√≠o es algo lento.\",\n",
                "            \"No recomiendo comprar aqu√≠. Me enviaron algo roto y no hay reembolso.\",\n",
                "            \"Amazon sigue siendo el mejor, aunque a veces los repartidores fallan.\",\n",
                "            \"Servicio t√©cnico inexistente, una odisea para devolver un producto.\"\n",
                "        ] * 10,\n",
                "        'puntuacion': [5, 1, 4, 1, 4, 2] * 10,\n",
                "        'fecha': pd.date_range(end=datetime.now(), periods=60).strftime('%Y-%m-%d').tolist(),\n",
                "        'longitud': [10, 15, 12, 14, 11, 13] * 10\n",
                "    })\n",
                "\n",
                "df_raw.to_csv('rese√±as_trustpilot_raw.csv', index=False, encoding='utf-8-sig')\n",
                "display(df_raw.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üßπ FASE 2: PREPROCESAMIENTO NLP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "nlp-preprocessor"
            },
            "outputs": [],
            "source": [
                "class SpanishTextPreprocessor:\n",
                "    \"\"\"\n",
                "    [Base: MONITOR_INTELIGENCIA_OPINION_v14 | Adaptaci√≥n: Rub√©n]\n",
                "    L√≥gica de limpieza profunda de texto para an√°lisis tem√°tico.\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        # [Juanes: Carga inicial de stopwords locales y dominio]\n",
                "        self.stopwords_es = set(stopwords.words('spanish'))\n",
                "        self.extra_stopwords = {\n",
                "            'amazon', 'bueno', 'malo', 'hacer', 'decir', 'ver', 'querer', \n",
                "            'cliente', 'pedido', 'producto', 'servicio', 'env√≠o', 'compra'\n",
                "        }\n",
                "        self.stopwords_es.update(self.extra_stopwords)\n",
                "        try:\n",
                "            self.nlp = spacy.load(\"es_core_news_sm\")\n",
                "        except:\n",
                "            self.nlp = None\n",
                "\n",
                "    def clean(self, text: str) -> str:\n",
                "        \"\"\"[Original: Limpieza b√°sica]\"\"\"\n",
                "        if not isinstance(text, str): return \"\"\n",
                "        text = text.lower()\n",
                "        text = re.sub(r'[^a-z√°√©√≠√≥√∫√º√±\\s]', ' ', text)\n",
                "        text = ' '.join(text.split())\n",
                "        return text\n",
                "\n",
                "    def remove_stopwords(self, text: str) -> str:\n",
                "        \"\"\"[Juanes: Filtrado selectivo]\"\"\"\n",
                "        tokens = word_tokenize(text)\n",
                "        return ' '.join([w for w in tokens if w not in self.stopwords_es and len(w) > 2])\n",
                "\n",
                "    def process_pipeline(self, text: str) -> str:\n",
                "        cleaned = self.clean(text)\n",
                "        no_stop = self.remove_stopwords(cleaned)\n",
                "        return no_stop\n",
                "\n",
                "print(\"üßº Procesando textos...\")\n",
                "preprocessor = SpanishTextPreprocessor()\n",
                "df_processed = df_raw.copy()\n",
                "df_processed['texto_limpio'] = [preprocessor.process_pipeline(t) for t in tqdm(df_processed['texto'])]\n",
                "print(\"‚úÖ Preprocesamiento completado.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üíé FASE 3: AN√ÅLISIS DE SENTIMIENTO Y VALOR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "sentiment-analysis"
            },
            "outputs": [],
            "source": [
                "class SentimentAnalyzerES:\n",
                "    \"\"\"\n",
                "    IA de Sentimiento H√≠brida.\n",
                "    [Base: MONITOR_INTELIGENCIA_OPINION_v14 | Novedad H√≠brida: Rub√©n]\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        self.pos_words = {'excelente', 'perfecto', 'genial', 'r√°pido', 'bueno', 'feliz', 'contento', 'recomiendo', '√∫til', 'eficiente'}\n",
                "        self.neg_words = {'p√©simo', 'horrible', 'terrible', 'error', 'malo', 'estafa', 'fraude', 'in√∫til', 'lento', 'decepci√≥n'}\n",
                "        self.translator = Translator() # [Rub√©n] Traductor para modelo superior\n",
                "\n",
                "    def analyze_simple(self, text: str) -> Dict:\n",
                "        \"\"\"[Juanes: M√©todo por diccionario local]\"\"\"\n",
                "        words = text.split()\n",
                "        pos = sum(1 for w in words if w in self.pos_words)\n",
                "        neg = sum(1 for w in words if w in self.neg_words)\n",
                "        total = pos + neg\n",
                "        if total == 0: return {'score': 0, 'label': 'neutral', 'conf': 0}\n",
                "        score = (pos - neg) / total\n",
                "        label = 'positivo' if score > 0.1 else ('negativo' if score < -0.1 else 'neutral')\n",
                "        return {'score': score, 'label': label, 'conf': abs(score)}\n",
                "\n",
                "    def analyze_textblob(self, text: str) -> Dict:\n",
                "        \"\"\"[Rub√©n: An√°lisis profundo v√≠a traducci√≥n]\"\"\"\n",
                "        try:\n",
                "            translated = self.translator.translate(text, src='es', dest='en').text\n",
                "            analysis = TextBlob(translated)\n",
                "            score = analysis.sentiment.polarity\n",
                "            label = 'positivo' if score > 0.1 else ('negativo' if score < -0.1 else 'neutral')\n",
                "            return {'score': score, 'label': label, 'conf': abs(score)}\n",
                "        except:\n",
                "            return self.analyze_simple(text)\n",
                "\n",
                "    def combined(self, text: str) -> Dict:\n",
                "        \"\"\"[Rub√©n: Orquestador H√≠brido]\"\"\"\n",
                "        res = self.analyze_simple(text)\n",
                "        # Si el texto es largo y el diccionario es d√©bil, usar IA profunda\n",
                "        if res['label'] == 'neutral' and len(text) > 40:\n",
                "            return self.analyze_textblob(text)\n",
                "        return res\n",
                "\n",
                "print(\"üòä Aplicando IA de sentimiento...\")\n",
                "analyzer = SentimentAnalyzerES()\n",
                "sentiments = [analyzer.combined(t) for t in tqdm(df_processed['texto_limpio'])]\n",
                "\n",
                "df_processed['sentimiento_score'] = [s['score'] for s in sentiments]\n",
                "df_processed['sentimiento'] = [s['label'] for s in sentiments]\n",
                "df_processed['confianza'] = [s['conf'] for s in sentiments]\n",
                "\n",
                "print(f\"\\nüìä RESUMEN DE SENTIMIENTO:\")\n",
                "print(df_processed['sentimiento'].value_counts(normalize=True) * 100)\n",
                "df_processed.to_csv('rese√±as_trustpilot_final.csv', index=False, encoding='utf-8-sig')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìä FASE 4: VISUALIZACI√ìN GR√ÅFICA (BI)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "visualizations"
            },
            "outputs": [],
            "source": [
                "# [Dise√±o base de Dashboards: Juanes | Enriquecimiento Temporal: Rub√©n]\n",
                "\n",
                "# 1. Configuraci√≥n de Gr√°ficos\n",
                "fig, axes = plt.subplots(3, 2, figsize=(18, 18))\n",
                "plt.subplots_adjust(hspace=0.4)\n",
                "\n",
                "# ‚òÅÔ∏è Gr√°fico 1: Nube de Palabras [Juanes]\n",
                "all_text = ' '.join(df_processed['texto_limpio'])\n",
                "if all_text:\n",
                "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='coolwarm').generate(all_text)\n",
                "    axes[0, 0].imshow(wordcloud, interpolation='bilinear')\n",
                "    axes[0, 0].set_title('üå•Ô∏è Temas m√°s mencionados [Juanes]', fontsize=15)\n",
                "    axes[0, 0].axis('off')\n",
                "\n",
                "# üìä Gr√°fico 2: Distribuci√≥n de Sentimiento [Juanes]\n",
                "colors = {'positivo': '#2ecc71', 'negativo': '#e74c3c', 'neutral': '#f1c40f'}\n",
                "sns.countplot(data=df_processed, x='sentimiento', palette=colors, ax=axes[0, 1])\n",
                "axes[0, 1].set_title('üìâ Salud de Marca (Distribuci√≥n) [Juanes]', fontsize=15)\n",
                "\n",
                "# üìà Gr√°fico 3: Correlaci√≥n Longitud vs Sentimiento [Juanes]\n",
                "sns.regplot(data=df_processed, x='longitud', y='sentimiento_score', ax=axes[1, 0], \n",
                "            scatter_kws={'alpha':0.5, 'color':'teal'}, line_kws={'color':'red'})\n",
                "axes[1, 0].set_title('üìè Detalle de Rese√±a vs Score [Juanes]', fontsize=15)\n",
                "\n",
                "# üè∑Ô∏è Gr√°fico 4: Top 10 Palabras [Juanes]\n",
                "top_words = Counter(all_text.split()).most_common(10)\n",
                "words_df = pd.DataFrame(top_words, columns=['palabra', 'frecuencia'])\n",
                "sns.barplot(data=words_df, x='frecuencia', y='palabra', ax=axes[1, 1], palette='viridis')\n",
                "axes[1, 1].set_title('üèÜ Top 10 Palabras Clave [Juanes]', fontsize=15)\n",
                "\n",
                "# üïí Gr√°fico 5: Evoluci√≥n Temporal del Sentimiento [Rub√©n]\n",
                "df_processed['fecha'] = pd.to_datetime(df_processed['fecha'])\n",
                "temporal = df_processed.groupby('fecha')['sentimiento_score'].mean().reset_index()\n",
                "sns.lineplot(data=temporal, x='fecha', y='sentimiento_score', ax=axes[2, 0], marker='o', color='purple')\n",
                "axes[2, 0].axhline(0, color='black', linestyle='--', alpha=0.3)\n",
                "axes[2, 0].set_title('üïí Evoluci√≥n de la Opini√≥n (Tendencias) [Rub√©n]', fontsize=15)\n",
                "axes[2, 0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# üåü Gr√°fico 6: Heatmap Estrellas vs Sentimiento [Rub√©n]\n",
                "matrix = pd.crosstab(df_processed['puntuacion'], df_processed['sentimiento'])\n",
                "sns.heatmap(matrix, annot=True, fmt='d', cmap='YlGnBu', ax=axes[2, 1])\n",
                "axes[2, 1].set_title('üî• Consistencia Estrellas vs IA [Rub√©n]', fontsize=15)\n",
                "\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
