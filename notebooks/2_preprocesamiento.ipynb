{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a55316",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # FASE 2: Preprocesamiento y Limpieza de Texto\n",
    "# ## Persona B - Procesamiento NLP\n",
    "\n",
    "# %%\n",
    "# Instalación de dependencias\n",
    "# !pip install nltk pandas matplotlib seaborn\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "# %%\n",
    "# Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from preprocessing import TextPreprocessor\n",
    "\n",
    "# Configurar estilo de gráficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %%\n",
    "# Cargar datos crudos\n",
    "print(\"Cargando datos crudos...\")\n",
    "df_raw = pd.read_csv('../data/raw/reviews_amazon_raw.csv')\n",
    "print(f\"Dataset cargado: {len(df_raw)} reseñas\")\n",
    "print(f\"Columnas: {list(df_raw.columns)}\")\n",
    "\n",
    "# %%\n",
    "# Mostrar información básica del dataset\n",
    "print(\"INFORMACIÓN DEL DATASET CRUDO\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total de reseñas: {len(df_raw)}\")\n",
    "print(f\"Reseñas con puntuación: {df_raw['puntuacion'].notna().sum()}\")\n",
    "print(f\"Reseñas sin puntuación: {df_raw['puntuacion'].isna().sum()}\")\n",
    "\n",
    "# Distribución de puntuaciones\n",
    "if df_raw['puntuacion'].notna().any():\n",
    "    print(\"\\nDistribución de puntuaciones:\")\n",
    "    print(df_raw['puntuacion'].value_counts().sort_index())\n",
    "\n",
    "# %%\n",
    "# Ejemplo de textos antes del procesamiento\n",
    "print(\"\\nEJEMPLOS DE TEXTO ANTES DEL PROCESAMIENTO:\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(3):\n",
    "    print(f\"\\nReseña {i+1} (Puntuación: {df_raw.loc[i, 'puntuacion']}):\")\n",
    "    print(f\"Texto: {df_raw.loc[i, 'texto'][:150]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# %%\n",
    "# Inicializar preprocesador\n",
    "print(\"\\nInicializando preprocesador de texto...\")\n",
    "preprocessor = TextPreprocessor(language='spanish')\n",
    "\n",
    "# %%\n",
    "# Procesar una reseña de ejemplo para ver el pipeline\n",
    "print(\"\\nPROCESAMIENTO DE UNA RESEÑA DE EJEMPLO:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_text = df_raw.loc[0, 'texto']\n",
    "print(f\"Texto original:\\n{sample_text}\\n\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result = preprocessor.preprocess_pipeline(sample_text, stem=True)\n",
    "\n",
    "print(f\"Texto limpio:\\n{result['texto_limpio']}\\n\")\n",
    "print(f\"Texto sin stopwords:\\n{result['texto_sin_stopwords']}\\n\")\n",
    "print(f\"Texto con stemming:\\n{result['texto_stemmed']}\\n\")\n",
    "print(f\"Tokens sin stopwords: {result['tokens_sin_stopwords'][:10]}...\")\n",
    "print(f\"Número de palabras original: {result['num_palabras_original']}\")\n",
    "print(f\"Número de palabras sin stopwords: {result['num_palabras_sin_stopwords']}\")\n",
    "\n",
    "# %%\n",
    "# Procesar todo el dataset\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROCESANDO TODO EL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_processed = preprocessor.process_dataframe(df_raw, text_column='texto')\n",
    "\n",
    "# %%\n",
    "# Calcular métricas adicionales\n",
    "df_processed = preprocessor.calculate_text_metrics(df_processed)\n",
    "\n",
    "# %%\n",
    "# Guardar dataset procesado\n",
    "output_path = '../data/processed/reviews_preprocessed.csv'\n",
    "df_processed.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"\\nDataset procesado guardado en: {output_path}\")\n",
    "\n",
    "# %%\n",
    "# Análisis de las métricas calculadas\n",
    "print(\"\\nANÁLISIS DE MÉTRICAS DE TEXTO\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Estadísticas de longitud\n",
    "print(\"\\nLongitud de textos:\")\n",
    "print(f\"  Original - Promedio: {df_processed['longitud_texto_original'].mean():.0f} chars\")\n",
    "print(f\"  Original - Máximo: {df_processed['longitud_texto_original'].max():.0f} chars\")\n",
    "print(f\"  Original - Mínimo: {df_processed['longitud_texto_original'].min():.0f} chars\")\n",
    "print(f\"  Limpio - Promedio: {df_processed['longitud_texto_limpio'].mean():.0f} chars\")\n",
    "\n",
    "print(\"\\nNúmero de palabras:\")\n",
    "print(f\"  Original - Promedio: {df_processed['num_palabras_original'].mean():.0f} palabras\")\n",
    "print(f\"  Limpio - Promedio: {df_processed['num_palabras_limpio'].mean():.0f} palabras\")\n",
    "print(f\"  Reducción por stopwords: {((df_processed['num_palabras_original'] - df_processed['num_palabras_sin_stopwords']).mean()):.1f} palabras/reseña\")\n",
    "\n",
    "print(f\"\\nDensidad léxica promedio: {df_processed['densidad_lexica'].mean():.2%}\")\n",
    "\n",
    "# %%\n",
    "# Visualización de métricas\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Distribución de longitud de textos originales\n",
    "axes[0, 0].hist(df_processed['longitud_texto_original'], bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribución Longitud Textos Originales')\n",
    "axes[0, 0].set_xlabel('Caracteres')\n",
    "axes[0, 0].set_ylabel('Frecuencia')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribución de longitud de textos limpios\n",
    "axes[0, 1].hist(df_processed['longitud_texto_limpio'], bins=30, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Distribución Longitud Textos Limpios')\n",
    "axes[0, 1].set_xlabel('Caracteres')\n",
    "axes[0, 1].set_ylabel('Frecuencia')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distribución de número de palabras\n",
    "axes[0, 2].hist(df_processed['num_palabras_original'], bins=30, alpha=0.5, label='Original', color='skyblue')\n",
    "axes[0, 2].hist(df_processed['num_palabras_sin_stopwords'], bins=30, alpha=0.5, label='Sin stopwords', color='lightcoral')\n",
    "axes[0, 2].set_title('Distribución Número de Palabras')\n",
    "axes[0, 2].set_xlabel('Número de palabras')\n",
    "axes[0, 2].set_ylabel('Frecuencia')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Densidad léxica por puntuación\n",
    "if 'puntuacion' in df_processed.columns and df_processed['puntuacion'].notna().any():\n",
    "    box_data = []\n",
    "    labels = []\n",
    "    for score in sorted(df_processed['puntuacion'].dropna().unique()):\n",
    "        mask = df_processed['puntuacion'] == score\n",
    "        box_data.append(df_processed.loc[mask, 'densidad_lexica'].dropna())\n",
    "        labels.append(f'{score} estrellas')\n",
    "    \n",
    "    axes[1, 0].boxplot(box_data, labels=labels)\n",
    "    axes[1, 0].set_title('Densidad Léxica por Puntuación')\n",
    "    axes[1, 0].set_ylabel('Densidad Léxica')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Relación entre longitud y puntuación\n",
    "if 'puntuacion' in df_processed.columns and df_processed['puntuacion'].notna().any():\n",
    "    scatter_data = df_processed.dropna(subset=['puntuacion'])\n",
    "    axes[1, 1].scatter(scatter_data['puntuacion'], scatter_data['num_palabras_original'], \n",
    "                       alpha=0.6, color='purple')\n",
    "    axes[1, 1].set_title('Longitud vs Puntuación')\n",
    "    axes[1, 1].set_xlabel('Puntuación (estrellas)')\n",
    "    axes[1, 1].set_ylabel('Número de palabras')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Distribución de densidad léxica\n",
    "axes[1, 2].hist(df_processed['densidad_lexica'], bins=30, alpha=0.7, color='gold')\n",
    "axes[1, 2].set_title('Distribución Densidad Léxica')\n",
    "axes[1, 2].set_xlabel('Densidad Léxica')\n",
    "axes[1, 2].set_ylabel('Frecuencia')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizations/metricas_preprocesamiento.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Análisis de stopwords eliminadas\n",
    "print(\"\\nANÁLISIS DE STOPWORDS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Contar frecuencia de palabras antes y después de eliminar stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Combinar todos los textos\n",
    "all_tokens_original = []\n",
    "all_tokens_clean = []\n",
    "\n",
    "for idx, row in df_processed.iterrows():\n",
    "    if isinstance(row['tokens'], list):\n",
    "        all_tokens_original.extend(row['tokens'])\n",
    "    if isinstance(row['tokens_sin_stopwords'], list):\n",
    "        all_tokens_clean.extend(row['tokens_sin_stopwords'])\n",
    "\n",
    "# Contar frecuencias\n",
    "freq_original = Counter(all_tokens_original)\n",
    "freq_clean = Counter(all_tokens_clean)\n",
    "\n",
    "print(f\"\\nTotal tokens originales: {len(all_tokens_original)}\")\n",
    "print(f\"Total tokens sin stopwords: {len(all_tokens_clean)}\")\n",
    "print(f\"Reducción: {((len(all_tokens_original) - len(all_tokens_clean)) / len(all_tokens_original) * 100):.1f}%\")\n",
    "\n",
    "# Mostrar las palabras más comunes antes de eliminar stopwords\n",
    "print(\"\\nTop 10 palabras más comunes (incluyendo stopwords):\")\n",
    "for word, count in freq_original.most_common(10):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 palabras más comunes (sin stopwords):\")\n",
    "for word, count in freq_clean.most_common(10):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "# %%\n",
    "# Preparar datos para la siguiente fase\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPARACIÓN PARA LA FASE 3: ANÁLISIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear dataset optimizado para análisis\n",
    "df_for_analysis = df_processed[[\n",
    "    'titulo', 'texto_original', 'texto_limpio', 'texto_sin_stopwords',\n",
    "    'texto_stemmed', 'puntuacion', 'fecha', 'usuario',\n",
    "    'num_palabras_original', 'num_palabras_sin_stopwords', 'densidad_lexica'\n",
    "]].copy()\n",
    "\n",
    "# Guardar dataset para análisis\n",
    "df_for_analysis.to_csv('../data/processed/reviews_for_analysis.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Dataset para análisis guardado: ../data/processed/reviews_for_analysis.csv\")\n",
    "print(f\"Número de registros: {len(df_for_analysis)}\")\n",
    "print(f\"Columnas: {list(df_for_analysis.columns)}\")\n",
    "\n",
    "# %%\n",
    "# Resumen del preprocesamiento\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMEN DEL PREPROCESAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Textos procesados: {len(df_processed)}\")\n",
    "print(f\"✓ Stopwords eliminadas: {len(preprocessor.stop_words)} palabras\")\n",
    "print(f\"✓ Reducción promedio de palabras por texto: {((df_processed['num_palabras_original'] - df_processed['num_palabras_sin_stopwords']).mean()):.1f}\")\n",
    "print(f\"✓ Densidad léxica promedio: {df_processed['densidad_lexica'].mean():.2%}\")\n",
    "print(f\"✓ Archivos generados:\")\n",
    "print(f\"   - reviews_preprocessed.csv (dataset completo)\")\n",
    "print(f\"   - reviews_for_analysis.csv (dataset optimizado)\")\n",
    "print(f\"   - metricas_preprocesamiento.png (visualizaciones)\")\n",
    "print(\"\\nEl dataset está listo para el análisis de sentimiento y frecuencia.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
