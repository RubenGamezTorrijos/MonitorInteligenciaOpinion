{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e3590",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../scripts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Importar el scraper\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscraper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrustpilotScraper\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Inicializar el scraper\u001b[39;00m\n\u001b[0;32m     22\u001b[0m scraper \u001b[38;5;241m=\u001b[39m TrustpilotScraper()\n",
      "File \u001b[1;32m~\\UEM_SSII_MonitorInteligenciaOpinion\\MonitorInteligenciaOpinion\\notebooks\\../scripts\\scraper.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # FASE 1: Web Scraping de Rese\u00f1as\n",
    "# ## Persona A (Organizador) - Extracci\u00f3n de Datos de Trustpilot\n",
    "\n",
    "# %%\n",
    "# Instalaci\u00f3n de dependencias (solo si es necesario)\n",
    "# !pip install requests beautifulsoup4 pandas\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# A\u00f1adir directorio de scripts al path\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "# %%\n",
    "# Importar el scraper\n",
    "from scraper import TrustpilotScraper\n",
    "\n",
    "# %%\n",
    "# Inicializar el scraper\n",
    "scraper = TrustpilotScraper()\n",
    "\n",
    "# %%\n",
    "# URL de Amazon Espa\u00f1a para an\u00e1lisis\n",
    "amazon_url = \"https://es.trustpilot.com/review/www.amazon.es\"\n",
    "\n",
    "# %%\n",
    "# Ejecutar scraping (2 p\u00e1ginas = ~40 rese\u00f1as)\n",
    "scraper.get_company_reviews(amazon_url, pages=2)\n",
    "\n",
    "# %%\n",
    "# Guardar datos crudos\n",
    "df_raw = scraper.save_to_csv('../data/raw/dataset_raw.csv')\n",
    "\n",
    "# %%\n",
    "# Mostrar informaci\u00f3n del dataset\n",
    "print(\"INFORMACI\u00d3N DEL DATASET CRUDO\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total de rese\u00f1as: {len(scraper.reviews_data)}\")\n",
    "print(f\"Columnas disponibles: {list(df_raw.columns)}\")\n",
    "\n",
    "# %%\n",
    "# Visualizar las primeras rese\u00f1as\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "print(\"\\nPRIMERAS 5 RESE\u00d1AS:\")\n",
    "print(df_raw[['usuario', 'puntuacion', 'texto_comentario_comentario']].head())\n",
    "\n",
    "# %%\n",
    "# An\u00e1lisis b\u00e1sico de los datos crudos\n",
    "print(\"\\nAN\u00c1LISIS B\u00c1SICO:\")\n",
    "print(f\"Rese\u00f1as con puntuaci\u00f3n: {df_raw['puntuacion'].notna().sum()}\")\n",
    "print(f\"Rese\u00f1as sin puntuaci\u00f3n: {df_raw['puntuacion'].isna().sum()}\")\n",
    "\n",
    "# %%\n",
    "# Distribuci\u00f3n de puntuaciones\n",
    "if df_raw['puntuacion'].notna().any():\n",
    "    print(\"\\nDISTRIBUCI\u00d3N DE PUNTUACIONES:\")\n",
    "    print(df_raw['puntuacion'].value_counts().sort_index())\n",
    "    \n",
    "    # Gr\u00e1fico simple de distribuci\u00f3n\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_raw['puntuacion'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
    "    plt.title('Distribuci\u00f3n de Puntuaciones en Rese\u00f1as de Amazon')\n",
    "    plt.xlabel('Estrellas (1-5)')\n",
    "    plt.ylabel('N\u00famero de Rese\u00f1as')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../visualizations/distribucion_puntuaciones_raw.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "# Estad\u00edsticas de longitud de texto_comentario_comentario\n",
    "df_raw['longitud_texto_comentario_comentario'] = df_raw['texto_comentario_comentario'].apply(len)\n",
    "print(\"\\nESTAD\u00cdSTICAS DE LONGITUD DE TEXTO:\")\n",
    "print(f\"Longitud m\u00e1xima: {df_raw['longitud_texto_comentario_comentario'].max()} caracteres\")\n",
    "print(f\"Longitud m\u00ednima: {df_raw['longitud_texto_comentario_comentario'].min()} caracteres\")\n",
    "print(f\"Longitud promedio: {df_raw['longitud_texto_comentario_comentario'].mean():.0f} caracteres\")\n",
    "\n",
    "# %%\n",
    "# Guardar metadatos del scraping\n",
    "metadata = {\n",
    "    'fecha_scraping': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_rese\u00f1as': len(df_raw),\n",
    "    'empresa': 'Amazon Espa\u00f1a',\n",
    "    'url_fuente': amazon_url,\n",
    "    'rese\u00f1as_con_puntuacion': df_raw['puntuacion'].notna().sum(),\n",
    "    'longitud_promedio_texto_comentario_comentario': round(df_raw['longitud_texto_comentario_comentario'].mean(), 2)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/raw/metadata_scraping.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nMETADATOS GUARDADOS:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# %%\n",
    "# Exportar dataset para la siguiente fase\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET CRUDO PREPARADO PARA LA FASE 2\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Archivo guardado: ../data/raw/dataset_raw.csv\")\n",
    "print(f\"Total registros: {len(df_raw)}\")\n",
    "print(\"\\nMuestra del dataset:\")\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f10337-1515-4fd9-bc1e-18c4eeab2379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "\"Python/Mu (mu_venv-38-20251118-212000)\"",
   "language": "python",
   "name": "mu_venv-38-20251118-212000"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}