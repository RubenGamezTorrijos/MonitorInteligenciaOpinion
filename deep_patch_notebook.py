import json
import re
from datetime import datetime

notebook_path = r'c:\Users\ruben\UEM_SSII_MonitorInteligenciaOpinion\MonitorInteligenciaOpinion\notebooks\Analisis_Amazon_TrustPilot_v6_DeepSeek_Revisar.ipynb'

with open(notebook_path, 'r', encoding='utf-8') as f:
    nb = json.load(f)

def update_cell_by_id(cell_id, new_source):
    for cell in nb['cells']:
        if cell.get('metadata', {}).get('id') == cell_id:
            cell['source'] = [line + ("" if line.endswith("\n") else "\n") for line in new_source]
            return True
    return False

# 1. Update Scraper Class (Selector Fix 2026 + Better Docs)
scraper_source = [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ”§ CLASE TRUSTPILOTSCRAPER - VersiÃ³n Optimizada 2026\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class TrustpilotScraper:\n",
    "    \"\"\"\n",
    "    Scraper de alto rendimiento para Trustpilot.\n",
    "    Autor: RubÃ©n\n",
    "    CaracterÃ­sticas:\n",
    "      - Selectores CSS actualizados a la estructura 2026.\n",
    "      - Manejo de sesiones y reintentos.\n",
    "      - Delay aleatorio Ã©tico para evitar bloqueos.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url: str, max_pages: int = 15):\n",
    "        self.base_url = base_url\n",
    "        self.max_pages = max_pages\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "            'Accept-Language': 'es-ES,es;q=0.9',\n",
    "        })\n",
    "        print(f\"âœ… Scraper inicializado para: {base_url}\")\n",
    "\n",
    "    def extract_review_data(self, review_element) -> Optional[Dict]:\n",
    "        \"\"\"Extrae datos estructurados de una card de reseÃ±a.\"\"\"\n",
    "        try:\n",
    "            # Selectores actualizados 2026\n",
    "            title_el = review_element.select_one('[data-review-title-typography=\"true\"]')\n",
    "            title = title_el.get_text(strip=True) if title_el else \"Sin tÃ­tulo\"\n",
    "\n",
    "            text_el = review_element.select_one('[data-review-content-typography=\"true\"]')\n",
    "            review_text = text_el.get_text(strip=True).replace(\"Ver mÃ¡s\", \"\").strip() if text_el else \"Texto no disponible\"\n",
    "\n",
    "            rating_container = review_element.select_one('[data-star-rating]')\n",
    "            rating = None\n",
    "            if rating_container:\n",
    "                rating_img = rating_container.find('img')\n",
    "                if rating_img and 'alt' in rating_img.attrs:\n",
    "                    match = re.search(r'(\\d+)', rating_img['alt'])\n",
    "                    rating = int(match.group(1)) if match else None\n",
    "\n",
    "            date_elem = review_element.find('time')\n",
    "            review_date = date_elem['datetime'] if date_elem and 'datetime' in date_elem.attrs else datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            user_elem = review_element.select_one('[data-consumer-name-typography=\"true\"]')\n",
    "            user_name = user_elem.get_text(strip=True) if user_elem else \"AnÃ³nimo\"\n",
    "\n",
    "            location_elem = review_element.select_one('[data-consumer-country-typography=\"true\"]')\n",
    "            if not location_elem: # Probamos selector secundario de ubicaciÃ³n\n",
    "                location_elem = review_element.select_one('div[class*=\"styles_consumerExtraDetails\"] span')\n",
    "            location = location_elem.get_text(strip=True) if location_elem else \"Desconocida\"\n",
    "\n",
    "            return {\n",
    "                'usuario': user_name,\n",
    "                'ubicacion': location,\n",
    "                'puntuacion': rating,\n",
    "                'titulo': title,\n",
    "                'texto_comentario': review_text,\n",
    "                'fecha': review_date,\n",
    "                'fecha_sistema': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def scrape_page(self, page_num: int) -> List[Dict]:\n",
    "        \"\"\"Extrae reseÃ±as de una pÃ¡gina individual.\"\"\"\n",
    "        url = f\"{self.base_url}?page={page_num}\" if page_num > 1 else self.base_url\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # El contenedor principal cambiÃ³ a 'article'\n",
    "            review_elements = soup.select('article[data-review-card-identifier]')\n",
    "            if not review_elements: review_elements = soup.find_all('article')\n",
    "            \n",
    "            reviews = []\n",
    "            for element in review_elements:\n",
    "                data = self.extract_review_data(element)\n",
    "                if data: reviews.append(data)\n",
    "            \n",
    "            # Delay aleatorio Ã©tico (Juanes: Implementado para evitar bloqueos)\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "            return reviews\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error en pÃ¡gina {page_num}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_all_pages(self) -> pd.DataFrame:\n",
    "        \"\"\"Orquestador principal de extracciÃ³n.\"\"\"\n",
    "        print(\"\\nðŸš€ INICIANDO EXTRACCIÃ“N MASIVA\")\n",
    "        all_reviews = []\n",
    "        target_reviews = 100\n",
    "\n",
    "        for page in range(1, self.max_pages + 1):\n",
    "            print(f\"   ðŸ“Š Progreso: {len(all_reviews)}/{target_reviews} reseÃ±as...\")\n",
    "            page_reviews = self.scrape_page(page)\n",
    "            if not page_reviews: break\n",
    "            all_reviews.extend(page_reviews)\n",
    "            if len(all_reviews) >= target_reviews: break\n",
    "        \n",
    "        df = pd.DataFrame(all_reviews)\n",
    "        if not df.empty:\n",
    "            # Persona B: Limpieza inicial de duplicados\n",
    "            df = df.drop_duplicates(subset=['usuario', 'texto_comentario', 'fecha'])\n",
    "        return df\n",
    "\n",
    "print(\"âœ… Clase TrustpilotScraper lista para su uso\")"
]
update_cell_by_id('clase-trustpilot-scraper', scraper_source)

# 2. Update NLP Preprocessor (Better spanish support + Robustness)
nlp_source = [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ”§ CLASE TEXTPREPROCESSOR - VersiÃ³n Optimizada por Juanes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import unicodedata\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Garantizar recursos NLTK\n",
    "        for res in ['punkt', 'stopwords']: \n",
    "            try: nltk.data.find(f'tokenizers/{res}' if res=='punkt' else f'corpora/{res}')\n",
    "            except: nltk.download(res, quiet=True)\n",
    "            \n",
    "        self.stop_words = set(stopwords.words('spanish'))\n",
    "        self.stemmer = SnowballStemmer('spanish')\n",
    "        self._add_custom_stopwords()\n",
    "        print(f\"âœ… Preprocesador listo. {len(self.stop_words)} stopwords cargadas.\")\n",
    "\n",
    "    def _add_custom_stopwords(self):\n",
    "        self.stop_words.update({'amazon', 'espaÃ±a', 'producto', 'compra', 'envio', 'pedido', 'hola', 'gracias'})\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text): return \"\"\n",
    "        text = str(text).lower()\n",
    "        # Eliminar acentos\n",
    "        text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "        # Solo letras\n",
    "        text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def full_pipeline(self, text: str) -> Dict:\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = word_tokenize(cleaned, language='spanish')\n",
    "        no_stop = [t for t in tokens if t not in self.stop_words and len(t) > 2]\n",
    "        \n",
    "        return {\n",
    "            'texto_original': text,\n",
    "            'texto_limpio': ' '.join(no_stop),\n",
    "            'tokens': no_stop,\n",
    "            'word_count_original': len(str(text).split()),\n",
    "            'word_count_clean': len(no_stop)\n",
    "        }\n",
    "\n",
    "print(\"âœ… Clase TextPreprocessor actualizada\")"
]
update_cell_by_id('clase-text-preprocessor', nlp_source)

# 3. Optimize Sentiment Analyzer (Better integration with Preprocessor)
sentiment_source = [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ˜Š CLASE SENTIMENTANALYZER - Implementado por RubÃ©n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from textblob import TextBlob\n",
    "from googletrans import Translator\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.translator = Translator()\n",
    "        self.lex_pos = {'excelente', 'bueno', 'rÃ¡pido', 'perfecto', 'recomiendo', 'satisfecho', 'genial'}\n",
    "        self.lex_neg = {'malo', 'pÃ©simo', 'lento', 'error', 'problema', 'decepcionado', 'terrible'}\n",
    "        print(\"âœ… Analizador de sentimiento inicializado\")\n",
    "\n",
    "    def analyze(self, text: str) -> float:\n",
    "        if not text or len(text) < 5: return 0.0\n",
    "        try:\n",
    "            # MÃ©todo 1: LÃ©xico simple (rÃ¡pido)\n",
    "            words = text.split()\n",
    "            score_lex = (sum(1 for w in words if w in self.lex_pos) - sum(1 for w in words if w in self.lex_neg)) / max(len(words), 1)\n",
    "            \n",
    "            # MÃ©todo 2: TextBlob (mÃ¡s preciso pero requiere traducciÃ³n)\n",
    "            # En Colab/entornos reales, limitamos traducciÃ³n para no saturar API\n",
    "            blob = TextBlob(str(text))\n",
    "            score_blob = blob.sentiment.polarity\n",
    "            \n",
    "            return (score_lex * 0.4) + (score_blob * 0.6)\n",
    "        except: return 0.0\n",
    "\n",
    "    def get_label(self, score: float) -> str:\n",
    "        if score > 0.1: return 'positivo'\n",
    "        if score < -0.1: return 'negativo'\n",
    "        return 'neutral'\n",
    "\n",
    "print(\"âœ… Analizador de sentimiento listo\")"
]
update_cell_by_id('clase-sentiment-analyzer', sentiment_source)

# 4. Actual execution cell (Crucial! Correcting column names from Fase 1)
execution_source = [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ PROCESAMIENTO E INSIGHTS - IntegraciÃ³n RubÃ©n y Juanes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Paso 1: Preprocesamiento NLP (Juanes)\n",
    "print(\"ðŸ”„ Aplicando limpieza NLP a las reseÃ±as...\")\n",
    "results = [preprocessor.full_pipeline(row['texto_comentario']) for _, row in df_reviews_raw.iterrows()]\n",
    "\n",
    "df_final = df_reviews_raw.copy()\n",
    "df_final['texto_limpio'] = [r['texto_limpio'] for r in results]\n",
    "df_final['num_palabras'] = [r['word_count_original'] for r in results]\n",
    "df_final['tokens'] = [r['tokens'] for r in results]\n",
    "\n",
    "# Paso 2: AnÃ¡lisis de Sentimiento (RubÃ©n)\n",
    "print(\"ðŸ˜Š Calculando sentimiento e inteligencia de opiniÃ³n...\")\n",
    "df_final['sentiment_score'] = df_final['texto_limpio'].apply(analyzer.analyze)\n",
    "df_final['sentimiento'] = df_final['sentiment_score'].apply(analyzer.get_label)\n",
    "\n",
    "print(f\"\\nâœ… Procesamiento completado: {len(df_final)} reseÃ±as analizadas.\")\n",
    "print(df_final[['usuario', 'puntuacion', 'sentimiento']].head())"
]
update_cell_by_id('aplicacion-preprocesamiento', execution_source)

# 5. Visualizations (Juanes - Fixing styles and paths)
viz_source = [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ¨ VISUALIZACIONES PROFESIONALES - Juanes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def generate_viz(df):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # 1. DistribuciÃ³n de Puntuaciones\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(data=df, x='puntuacion', palette='RdYlGn')\n",
    "    plt.title('DistribuciÃ³n de Estrellas (Trustpilot)', fontsize=15)\n",
    "    \n",
    "    # 2. DistribuciÃ³n de Sentimiento\n",
    "    plt.subplot(1, 2, 2)\n",
    "    df['sentimiento'].value_counts().plot.pie(autopct='%1.1f%%', colors=['#2ecc71', '#f1c40f', '#e74c3c'])\n",
    "    plt.title('Inteligencia de OpiniÃ³n (Sentimiento)', fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('analisis_resumen.png')\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Nube de Palabras\n",
    "    text = ' '.join(df['texto_limpio'].dropna())\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.title('Temas mÃ¡s recurrentes en las reseÃ±as', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "generate_viz(df_final)\n",
    "print(\"âœ… GrÃ¡ficos generados correctamente\")"
]
update_cell_by_id('ejecucion-visualizaciones', viz_source)

# Update metadata for Colab compatibility
nb['metadata']['colab'] = {\"provenance\": []}

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=2, ensure_ascii=False)

print("Notebook 'Analisis_Amazon_TrustPilot_v6_DeepSeek_Revisar.ipynb' fully updated and documented.")
